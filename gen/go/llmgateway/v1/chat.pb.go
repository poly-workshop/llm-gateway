// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.8
// 	protoc        (unknown)
// source: llmgateway/v1/chat.proto

package llmgatewayv1

import (
	_ "google.golang.org/genproto/googleapis/api/annotations"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	structpb "google.golang.org/protobuf/types/known/structpb"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// ImageURL represents an image URL with optional detail level for vision models.
type ImageURL struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The URL of the image. Can be a regular URL or a base64 data URL.
	Url string `protobuf:"bytes,1,opt,name=url,proto3" json:"url,omitempty"`
	// Optional detail level: "auto", "low", or "high". Defaults to "auto".
	Detail        string `protobuf:"bytes,2,opt,name=detail,proto3" json:"detail,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ImageURL) Reset() {
	*x = ImageURL{}
	mi := &file_llmgateway_v1_chat_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ImageURL) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ImageURL) ProtoMessage() {}

func (x *ImageURL) ProtoReflect() protoreflect.Message {
	mi := &file_llmgateway_v1_chat_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ImageURL.ProtoReflect.Descriptor instead.
func (*ImageURL) Descriptor() ([]byte, []int) {
	return file_llmgateway_v1_chat_proto_rawDescGZIP(), []int{0}
}

func (x *ImageURL) GetUrl() string {
	if x != nil {
		return x.Url
	}
	return ""
}

func (x *ImageURL) GetDetail() string {
	if x != nil {
		return x.Detail
	}
	return ""
}

// ContentPart represents a part of a multimodal message content.
type ContentPart struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The type of content: "text" or "image_url".
	Type string `protobuf:"bytes,1,opt,name=type,proto3" json:"type,omitempty"`
	// Text content (when type = "text").
	Text string `protobuf:"bytes,2,opt,name=text,proto3" json:"text,omitempty"`
	// Image URL content (when type = "image_url").
	ImageUrl      *ImageURL `protobuf:"bytes,3,opt,name=image_url,json=imageUrl,proto3" json:"image_url,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ContentPart) Reset() {
	*x = ContentPart{}
	mi := &file_llmgateway_v1_chat_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ContentPart) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ContentPart) ProtoMessage() {}

func (x *ContentPart) ProtoReflect() protoreflect.Message {
	mi := &file_llmgateway_v1_chat_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ContentPart.ProtoReflect.Descriptor instead.
func (*ContentPart) Descriptor() ([]byte, []int) {
	return file_llmgateway_v1_chat_proto_rawDescGZIP(), []int{1}
}

func (x *ContentPart) GetType() string {
	if x != nil {
		return x.Type
	}
	return ""
}

func (x *ContentPart) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *ContentPart) GetImageUrl() *ImageURL {
	if x != nil {
		return x.ImageUrl
	}
	return nil
}

// One chat message (OpenAI-style).
// Supports both simple text content and multimodal content (text + images).
type ChatMessage struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// e.g. "system", "user", "assistant", "tool".
	Role string `protobuf:"bytes,1,opt,name=role,proto3" json:"role,omitempty"`
	// Content can be either a string (for text-only) or an array of ContentPart (for vision).
	// In JSON: "content": "hello" or "content": [{"type": "text", "text": "hello"}, {"type": "image_url", ...}]
	Content       *structpb.Value `protobuf:"bytes,2,opt,name=content,proto3" json:"content,omitempty"`
	Name          string          `protobuf:"bytes,3,opt,name=name,proto3" json:"name,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ChatMessage) Reset() {
	*x = ChatMessage{}
	mi := &file_llmgateway_v1_chat_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatMessage) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatMessage) ProtoMessage() {}

func (x *ChatMessage) ProtoReflect() protoreflect.Message {
	mi := &file_llmgateway_v1_chat_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatMessage.ProtoReflect.Descriptor instead.
func (*ChatMessage) Descriptor() ([]byte, []int) {
	return file_llmgateway_v1_chat_proto_rawDescGZIP(), []int{2}
}

func (x *ChatMessage) GetRole() string {
	if x != nil {
		return x.Role
	}
	return ""
}

func (x *ChatMessage) GetContent() *structpb.Value {
	if x != nil {
		return x.Content
	}
	return nil
}

func (x *ChatMessage) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

type TokenUsage struct {
	state            protoimpl.MessageState `protogen:"open.v1"`
	PromptTokens     uint32                 `protobuf:"varint,1,opt,name=prompt_tokens,json=promptTokens,proto3" json:"prompt_tokens,omitempty"`
	CompletionTokens uint32                 `protobuf:"varint,2,opt,name=completion_tokens,json=completionTokens,proto3" json:"completion_tokens,omitempty"`
	TotalTokens      uint32                 `protobuf:"varint,3,opt,name=total_tokens,json=totalTokens,proto3" json:"total_tokens,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *TokenUsage) Reset() {
	*x = TokenUsage{}
	mi := &file_llmgateway_v1_chat_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TokenUsage) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TokenUsage) ProtoMessage() {}

func (x *TokenUsage) ProtoReflect() protoreflect.Message {
	mi := &file_llmgateway_v1_chat_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TokenUsage.ProtoReflect.Descriptor instead.
func (*TokenUsage) Descriptor() ([]byte, []int) {
	return file_llmgateway_v1_chat_proto_rawDescGZIP(), []int{3}
}

func (x *TokenUsage) GetPromptTokens() uint32 {
	if x != nil {
		return x.PromptTokens
	}
	return 0
}

func (x *TokenUsage) GetCompletionTokens() uint32 {
	if x != nil {
		return x.CompletionTokens
	}
	return 0
}

func (x *TokenUsage) GetTotalTokens() uint32 {
	if x != nil {
		return x.TotalTokens
	}
	return 0
}

type ChatCompletionChoice struct {
	state   protoimpl.MessageState `protogen:"open.v1"`
	Index   uint32                 `protobuf:"varint,1,opt,name=index,proto3" json:"index,omitempty"`
	Message *ChatMessage           `protobuf:"bytes,2,opt,name=message,proto3" json:"message,omitempty"`
	// e.g. "stop", "length", "tool_calls".
	FinishReason  string `protobuf:"bytes,3,opt,name=finish_reason,json=finishReason,proto3" json:"finish_reason,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ChatCompletionChoice) Reset() {
	*x = ChatCompletionChoice{}
	mi := &file_llmgateway_v1_chat_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatCompletionChoice) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatCompletionChoice) ProtoMessage() {}

func (x *ChatCompletionChoice) ProtoReflect() protoreflect.Message {
	mi := &file_llmgateway_v1_chat_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatCompletionChoice.ProtoReflect.Descriptor instead.
func (*ChatCompletionChoice) Descriptor() ([]byte, []int) {
	return file_llmgateway_v1_chat_proto_rawDescGZIP(), []int{4}
}

func (x *ChatCompletionChoice) GetIndex() uint32 {
	if x != nil {
		return x.Index
	}
	return 0
}

func (x *ChatCompletionChoice) GetMessage() *ChatMessage {
	if x != nil {
		return x.Message
	}
	return nil
}

func (x *ChatCompletionChoice) GetFinishReason() string {
	if x != nil {
		return x.FinishReason
	}
	return ""
}

type CreateChatCompletionRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Routed model id, e.g. "openai/gpt-4.1-mini".
	Model    string         `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	Messages []*ChatMessage `protobuf:"bytes,2,rep,name=messages,proto3" json:"messages,omitempty"`
	// Optional tuning knobs (minimal subset).
	Temperature float64 `protobuf:"fixed64,3,opt,name=temperature,proto3" json:"temperature,omitempty"`
	MaxTokens   uint32  `protobuf:"varint,4,opt,name=max_tokens,json=maxTokens,proto3" json:"max_tokens,omitempty"`
	// Optional user identifier for analytics/rate-limit.
	User          string `protobuf:"bytes,5,opt,name=user,proto3" json:"user,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CreateChatCompletionRequest) Reset() {
	*x = CreateChatCompletionRequest{}
	mi := &file_llmgateway_v1_chat_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CreateChatCompletionRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CreateChatCompletionRequest) ProtoMessage() {}

func (x *CreateChatCompletionRequest) ProtoReflect() protoreflect.Message {
	mi := &file_llmgateway_v1_chat_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CreateChatCompletionRequest.ProtoReflect.Descriptor instead.
func (*CreateChatCompletionRequest) Descriptor() ([]byte, []int) {
	return file_llmgateway_v1_chat_proto_rawDescGZIP(), []int{5}
}

func (x *CreateChatCompletionRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *CreateChatCompletionRequest) GetMessages() []*ChatMessage {
	if x != nil {
		return x.Messages
	}
	return nil
}

func (x *CreateChatCompletionRequest) GetTemperature() float64 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *CreateChatCompletionRequest) GetMaxTokens() uint32 {
	if x != nil {
		return x.MaxTokens
	}
	return 0
}

func (x *CreateChatCompletionRequest) GetUser() string {
	if x != nil {
		return x.User
	}
	return ""
}

type CreateChatCompletionResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	Id    string                 `protobuf:"bytes,1,opt,name=id,proto3" json:"id,omitempty"`
	// unix seconds
	Created       int64                   `protobuf:"varint,2,opt,name=created,proto3" json:"created,omitempty"`
	Model         string                  `protobuf:"bytes,3,opt,name=model,proto3" json:"model,omitempty"`
	Choices       []*ChatCompletionChoice `protobuf:"bytes,4,rep,name=choices,proto3" json:"choices,omitempty"`
	Usage         *TokenUsage             `protobuf:"bytes,5,opt,name=usage,proto3" json:"usage,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CreateChatCompletionResponse) Reset() {
	*x = CreateChatCompletionResponse{}
	mi := &file_llmgateway_v1_chat_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CreateChatCompletionResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CreateChatCompletionResponse) ProtoMessage() {}

func (x *CreateChatCompletionResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llmgateway_v1_chat_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CreateChatCompletionResponse.ProtoReflect.Descriptor instead.
func (*CreateChatCompletionResponse) Descriptor() ([]byte, []int) {
	return file_llmgateway_v1_chat_proto_rawDescGZIP(), []int{6}
}

func (x *CreateChatCompletionResponse) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *CreateChatCompletionResponse) GetCreated() int64 {
	if x != nil {
		return x.Created
	}
	return 0
}

func (x *CreateChatCompletionResponse) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *CreateChatCompletionResponse) GetChoices() []*ChatCompletionChoice {
	if x != nil {
		return x.Choices
	}
	return nil
}

func (x *CreateChatCompletionResponse) GetUsage() *TokenUsage {
	if x != nil {
		return x.Usage
	}
	return nil
}

type CreateChatCompletionStreamRequest struct {
	state         protoimpl.MessageState       `protogen:"open.v1"`
	Request       *CreateChatCompletionRequest `protobuf:"bytes,1,opt,name=request,proto3" json:"request,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CreateChatCompletionStreamRequest) Reset() {
	*x = CreateChatCompletionStreamRequest{}
	mi := &file_llmgateway_v1_chat_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CreateChatCompletionStreamRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CreateChatCompletionStreamRequest) ProtoMessage() {}

func (x *CreateChatCompletionStreamRequest) ProtoReflect() protoreflect.Message {
	mi := &file_llmgateway_v1_chat_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CreateChatCompletionStreamRequest.ProtoReflect.Descriptor instead.
func (*CreateChatCompletionStreamRequest) Descriptor() ([]byte, []int) {
	return file_llmgateway_v1_chat_proto_rawDescGZIP(), []int{7}
}

func (x *CreateChatCompletionStreamRequest) GetRequest() *CreateChatCompletionRequest {
	if x != nil {
		return x.Request
	}
	return nil
}

// Streaming chunk shape (minimal).
type CreateChatCompletionStreamResponse struct {
	state         protoimpl.MessageState              `protogen:"open.v1"`
	Id            string                              `protobuf:"bytes,1,opt,name=id,proto3" json:"id,omitempty"`
	Created       int64                               `protobuf:"varint,2,opt,name=created,proto3" json:"created,omitempty"`
	Model         string                              `protobuf:"bytes,3,opt,name=model,proto3" json:"model,omitempty"`
	Choices       []*CreateChatCompletionStreamChoice `protobuf:"bytes,4,rep,name=choices,proto3" json:"choices,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CreateChatCompletionStreamResponse) Reset() {
	*x = CreateChatCompletionStreamResponse{}
	mi := &file_llmgateway_v1_chat_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CreateChatCompletionStreamResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CreateChatCompletionStreamResponse) ProtoMessage() {}

func (x *CreateChatCompletionStreamResponse) ProtoReflect() protoreflect.Message {
	mi := &file_llmgateway_v1_chat_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CreateChatCompletionStreamResponse.ProtoReflect.Descriptor instead.
func (*CreateChatCompletionStreamResponse) Descriptor() ([]byte, []int) {
	return file_llmgateway_v1_chat_proto_rawDescGZIP(), []int{8}
}

func (x *CreateChatCompletionStreamResponse) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *CreateChatCompletionStreamResponse) GetCreated() int64 {
	if x != nil {
		return x.Created
	}
	return 0
}

func (x *CreateChatCompletionStreamResponse) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *CreateChatCompletionStreamResponse) GetChoices() []*CreateChatCompletionStreamChoice {
	if x != nil {
		return x.Choices
	}
	return nil
}

type CreateChatCompletionStreamChoice struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	Index         uint32                 `protobuf:"varint,1,opt,name=index,proto3" json:"index,omitempty"`
	Delta         *ChatCompletionDelta   `protobuf:"bytes,2,opt,name=delta,proto3" json:"delta,omitempty"`
	FinishReason  string                 `protobuf:"bytes,3,opt,name=finish_reason,json=finishReason,proto3" json:"finish_reason,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CreateChatCompletionStreamChoice) Reset() {
	*x = CreateChatCompletionStreamChoice{}
	mi := &file_llmgateway_v1_chat_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CreateChatCompletionStreamChoice) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CreateChatCompletionStreamChoice) ProtoMessage() {}

func (x *CreateChatCompletionStreamChoice) ProtoReflect() protoreflect.Message {
	mi := &file_llmgateway_v1_chat_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CreateChatCompletionStreamChoice.ProtoReflect.Descriptor instead.
func (*CreateChatCompletionStreamChoice) Descriptor() ([]byte, []int) {
	return file_llmgateway_v1_chat_proto_rawDescGZIP(), []int{9}
}

func (x *CreateChatCompletionStreamChoice) GetIndex() uint32 {
	if x != nil {
		return x.Index
	}
	return 0
}

func (x *CreateChatCompletionStreamChoice) GetDelta() *ChatCompletionDelta {
	if x != nil {
		return x.Delta
	}
	return nil
}

func (x *CreateChatCompletionStreamChoice) GetFinishReason() string {
	if x != nil {
		return x.FinishReason
	}
	return ""
}

type ChatCompletionDelta struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// In practice you may only stream content deltas; role may appear in the first chunk.
	Role          string `protobuf:"bytes,1,opt,name=role,proto3" json:"role,omitempty"`
	Content       string `protobuf:"bytes,2,opt,name=content,proto3" json:"content,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ChatCompletionDelta) Reset() {
	*x = ChatCompletionDelta{}
	mi := &file_llmgateway_v1_chat_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatCompletionDelta) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatCompletionDelta) ProtoMessage() {}

func (x *ChatCompletionDelta) ProtoReflect() protoreflect.Message {
	mi := &file_llmgateway_v1_chat_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatCompletionDelta.ProtoReflect.Descriptor instead.
func (*ChatCompletionDelta) Descriptor() ([]byte, []int) {
	return file_llmgateway_v1_chat_proto_rawDescGZIP(), []int{10}
}

func (x *ChatCompletionDelta) GetRole() string {
	if x != nil {
		return x.Role
	}
	return ""
}

func (x *ChatCompletionDelta) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

var File_llmgateway_v1_chat_proto protoreflect.FileDescriptor

const file_llmgateway_v1_chat_proto_rawDesc = "" +
	"\n" +
	"\x18llmgateway/v1/chat.proto\x12\rllmgateway.v1\x1a\x1fgoogle/api/field_behavior.proto\x1a\x1cgoogle/protobuf/struct.proto\"9\n" +
	"\bImageURL\x12\x15\n" +
	"\x03url\x18\x01 \x01(\tB\x03\xe0A\x02R\x03url\x12\x16\n" +
	"\x06detail\x18\x02 \x01(\tR\x06detail\"p\n" +
	"\vContentPart\x12\x17\n" +
	"\x04type\x18\x01 \x01(\tB\x03\xe0A\x02R\x04type\x12\x12\n" +
	"\x04text\x18\x02 \x01(\tR\x04text\x124\n" +
	"\timage_url\x18\x03 \x01(\v2\x17.llmgateway.v1.ImageURLR\bimageUrl\"l\n" +
	"\vChatMessage\x12\x17\n" +
	"\x04role\x18\x01 \x01(\tB\x03\xe0A\x02R\x04role\x120\n" +
	"\acontent\x18\x02 \x01(\v2\x16.google.protobuf.ValueR\acontent\x12\x12\n" +
	"\x04name\x18\x03 \x01(\tR\x04name\"\x81\x01\n" +
	"\n" +
	"TokenUsage\x12#\n" +
	"\rprompt_tokens\x18\x01 \x01(\rR\fpromptTokens\x12+\n" +
	"\x11completion_tokens\x18\x02 \x01(\rR\x10completionTokens\x12!\n" +
	"\ftotal_tokens\x18\x03 \x01(\rR\vtotalTokens\"\x87\x01\n" +
	"\x14ChatCompletionChoice\x12\x14\n" +
	"\x05index\x18\x01 \x01(\rR\x05index\x124\n" +
	"\amessage\x18\x02 \x01(\v2\x1a.llmgateway.v1.ChatMessageR\amessage\x12#\n" +
	"\rfinish_reason\x18\x03 \x01(\tR\ffinishReason\"\xca\x01\n" +
	"\x1bCreateChatCompletionRequest\x12\x19\n" +
	"\x05model\x18\x01 \x01(\tB\x03\xe0A\x02R\x05model\x12;\n" +
	"\bmessages\x18\x02 \x03(\v2\x1a.llmgateway.v1.ChatMessageB\x03\xe0A\x02R\bmessages\x12 \n" +
	"\vtemperature\x18\x03 \x01(\x01R\vtemperature\x12\x1d\n" +
	"\n" +
	"max_tokens\x18\x04 \x01(\rR\tmaxTokens\x12\x12\n" +
	"\x04user\x18\x05 \x01(\tR\x04user\"\xce\x01\n" +
	"\x1cCreateChatCompletionResponse\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\tR\x02id\x12\x18\n" +
	"\acreated\x18\x02 \x01(\x03R\acreated\x12\x14\n" +
	"\x05model\x18\x03 \x01(\tR\x05model\x12=\n" +
	"\achoices\x18\x04 \x03(\v2#.llmgateway.v1.ChatCompletionChoiceR\achoices\x12/\n" +
	"\x05usage\x18\x05 \x01(\v2\x19.llmgateway.v1.TokenUsageR\x05usage\"n\n" +
	"!CreateChatCompletionStreamRequest\x12I\n" +
	"\arequest\x18\x01 \x01(\v2*.llmgateway.v1.CreateChatCompletionRequestB\x03\xe0A\x02R\arequest\"\xaf\x01\n" +
	"\"CreateChatCompletionStreamResponse\x12\x0e\n" +
	"\x02id\x18\x01 \x01(\tR\x02id\x12\x18\n" +
	"\acreated\x18\x02 \x01(\x03R\acreated\x12\x14\n" +
	"\x05model\x18\x03 \x01(\tR\x05model\x12I\n" +
	"\achoices\x18\x04 \x03(\v2/.llmgateway.v1.CreateChatCompletionStreamChoiceR\achoices\"\x97\x01\n" +
	" CreateChatCompletionStreamChoice\x12\x14\n" +
	"\x05index\x18\x01 \x01(\rR\x05index\x128\n" +
	"\x05delta\x18\x02 \x01(\v2\".llmgateway.v1.ChatCompletionDeltaR\x05delta\x12#\n" +
	"\rfinish_reason\x18\x03 \x01(\tR\ffinishReason\"C\n" +
	"\x13ChatCompletionDelta\x12\x12\n" +
	"\x04role\x18\x01 \x01(\tR\x04role\x12\x18\n" +
	"\acontent\x18\x02 \x01(\tR\acontentBHZFgithub.com/poly-workshop/llm-gateway/gen/go/llmgateway/v1;llmgatewayv1b\x06proto3"

var (
	file_llmgateway_v1_chat_proto_rawDescOnce sync.Once
	file_llmgateway_v1_chat_proto_rawDescData []byte
)

func file_llmgateway_v1_chat_proto_rawDescGZIP() []byte {
	file_llmgateway_v1_chat_proto_rawDescOnce.Do(func() {
		file_llmgateway_v1_chat_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_llmgateway_v1_chat_proto_rawDesc), len(file_llmgateway_v1_chat_proto_rawDesc)))
	})
	return file_llmgateway_v1_chat_proto_rawDescData
}

var file_llmgateway_v1_chat_proto_msgTypes = make([]protoimpl.MessageInfo, 11)
var file_llmgateway_v1_chat_proto_goTypes = []any{
	(*ImageURL)(nil),                           // 0: llmgateway.v1.ImageURL
	(*ContentPart)(nil),                        // 1: llmgateway.v1.ContentPart
	(*ChatMessage)(nil),                        // 2: llmgateway.v1.ChatMessage
	(*TokenUsage)(nil),                         // 3: llmgateway.v1.TokenUsage
	(*ChatCompletionChoice)(nil),               // 4: llmgateway.v1.ChatCompletionChoice
	(*CreateChatCompletionRequest)(nil),        // 5: llmgateway.v1.CreateChatCompletionRequest
	(*CreateChatCompletionResponse)(nil),       // 6: llmgateway.v1.CreateChatCompletionResponse
	(*CreateChatCompletionStreamRequest)(nil),  // 7: llmgateway.v1.CreateChatCompletionStreamRequest
	(*CreateChatCompletionStreamResponse)(nil), // 8: llmgateway.v1.CreateChatCompletionStreamResponse
	(*CreateChatCompletionStreamChoice)(nil),   // 9: llmgateway.v1.CreateChatCompletionStreamChoice
	(*ChatCompletionDelta)(nil),                // 10: llmgateway.v1.ChatCompletionDelta
	(*structpb.Value)(nil),                     // 11: google.protobuf.Value
}
var file_llmgateway_v1_chat_proto_depIdxs = []int32{
	0,  // 0: llmgateway.v1.ContentPart.image_url:type_name -> llmgateway.v1.ImageURL
	11, // 1: llmgateway.v1.ChatMessage.content:type_name -> google.protobuf.Value
	2,  // 2: llmgateway.v1.ChatCompletionChoice.message:type_name -> llmgateway.v1.ChatMessage
	2,  // 3: llmgateway.v1.CreateChatCompletionRequest.messages:type_name -> llmgateway.v1.ChatMessage
	4,  // 4: llmgateway.v1.CreateChatCompletionResponse.choices:type_name -> llmgateway.v1.ChatCompletionChoice
	3,  // 5: llmgateway.v1.CreateChatCompletionResponse.usage:type_name -> llmgateway.v1.TokenUsage
	5,  // 6: llmgateway.v1.CreateChatCompletionStreamRequest.request:type_name -> llmgateway.v1.CreateChatCompletionRequest
	9,  // 7: llmgateway.v1.CreateChatCompletionStreamResponse.choices:type_name -> llmgateway.v1.CreateChatCompletionStreamChoice
	10, // 8: llmgateway.v1.CreateChatCompletionStreamChoice.delta:type_name -> llmgateway.v1.ChatCompletionDelta
	9,  // [9:9] is the sub-list for method output_type
	9,  // [9:9] is the sub-list for method input_type
	9,  // [9:9] is the sub-list for extension type_name
	9,  // [9:9] is the sub-list for extension extendee
	0,  // [0:9] is the sub-list for field type_name
}

func init() { file_llmgateway_v1_chat_proto_init() }
func file_llmgateway_v1_chat_proto_init() {
	if File_llmgateway_v1_chat_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_llmgateway_v1_chat_proto_rawDesc), len(file_llmgateway_v1_chat_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   11,
			NumExtensions: 0,
			NumServices:   0,
		},
		GoTypes:           file_llmgateway_v1_chat_proto_goTypes,
		DependencyIndexes: file_llmgateway_v1_chat_proto_depIdxs,
		MessageInfos:      file_llmgateway_v1_chat_proto_msgTypes,
	}.Build()
	File_llmgateway_v1_chat_proto = out.File
	file_llmgateway_v1_chat_proto_goTypes = nil
	file_llmgateway_v1_chat_proto_depIdxs = nil
}
